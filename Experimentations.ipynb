{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4Z-pYPSkISYg"
      ],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Next word prediction using LSTM + Attention(Trial 1)"
      ],
      "metadata": {
        "id": "4Z-pYPSkISYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "jKokdSMSIS43"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[\"“”]', '\"', text)\n",
        "    text = re.sub(r\"[’‘']\", \"'\", text)\n",
        "    text = re.sub(r'[—–]', '-', text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\'\\\"\\-\\(\\)]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "with open('sherlock_holmes.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "start_idx = raw_text.find(\"*** START OF\")\n",
        "end_idx = raw_text.find(\"*** END OF\")\n",
        "text = raw_text[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else raw_text\n",
        "text = clean_text(text)"
      ],
      "metadata": {
        "id": "JCDTh3YNINRk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset"
      ],
      "metadata": {
        "id": "NHpVz9hPfxJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "UqFhWuh9f4no"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_sherlock_holmes():\n",
        "    url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "    filename = \"sherlock_holmes.txt\" # store it for easy access\n",
        "\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"Downloading Sherlock Holmes text...\")\n",
        "        r = requests.get(url, timeout=30)\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(r.text)\n",
        "        print(\"Downloaded and saved as\", filename)\n",
        "    else:\n",
        "        print(\"Using cached file:\", filename)"
      ],
      "metadata": {
        "id": "ReoZ6clOf64K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[\"“”]', '\"', text)\n",
        "    text = re.sub(r\"[’‘']\", \"'\", text)\n",
        "    text = re.sub(r'[—–]', '-', text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\'\\\"\\-\\(\\)]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "download_sherlock_holmes()\n",
        "\n",
        "with open('sherlock_holmes.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "start_idx = raw_text.find(\"*** START OF\")\n",
        "end_idx = raw_text.find(\"*** END OF\")\n",
        "text = raw_text[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else raw_text\n",
        "text = clean_text(text)\n",
        "\n",
        "\n",
        "# Add special tokens for sequence control\n",
        "START_TOKEN = '<START>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "\n",
        "def proper_sentence_split(text):\n",
        "    \"\"\"Better sentence splitting that handles punctuation correctly\"\"\"\n",
        "    # Split on sentence-ending punctuation, but keep the punctuation\n",
        "    sentences = re.split(r'([.!?]+)', text)\n",
        "\n",
        "    # Recombine sentences with their punctuation\n",
        "    result = []\n",
        "    for i in range(0, len(sentences) - 1, 2):\n",
        "        sentence = sentences[i].strip()\n",
        "        punct = sentences[i + 1] if i + 1 < len(sentences) else ''\n",
        "        if sentence:  # Only add non-empty sentences\n",
        "            result.append(sentence + punct)\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_sliding_window_sequences(text, window_size=10):\n",
        "    \"\"\"Create overlapping sequences using sliding window approach\"\"\"\n",
        "    tokens = text.split()\n",
        "    sequences = []\n",
        "\n",
        "    # Create overlapping sequences across the entire text\n",
        "    for i in range(len(tokens) - window_size + 1):\n",
        "        sequence = tokens[i:i + window_size]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "print(\"Creating vocabulary from tokens...\")\n",
        "tokens = text.split()\n",
        "vocab = ['<PAD>', '<UNK>', START_TOKEN, EOS_TOKEN] + sorted(set(tokens))\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx2word = {i: word for word, i in word2idx.items()}\n",
        "\n",
        "# Get indices for special tokens\n",
        "start_idx = word2idx[START_TOKEN]\n",
        "eos_idx = word2idx[EOS_TOKEN]\n",
        "print(f\"START token index: {start_idx}, EOS token index: {eos_idx}\")\n",
        "\n",
        "print(\"Generating sequences with cross-sentence context...\")\n",
        "\n",
        "# Method 1: Sliding window across entire text (for cross-sentence learning)\n",
        "sliding_sequences = create_sliding_window_sequences(text, window_size=15)\n",
        "\n",
        "# Method 2: Proper sentence-based sequences with START/EOS tokens\n",
        "sentences = proper_sentence_split(text)\n",
        "sentence_sequences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence_words = sentence.strip().split()\n",
        "    if len(sentence_words) >= 2:  # Only process sentences with at least 2 words\n",
        "        # Add START and EOS tokens\n",
        "        tokenized = [START_TOKEN] + sentence_words + [EOS_TOKEN]\n",
        "        # Create progressive sequences\n",
        "        for i in range(2, len(tokenized) + 1):\n",
        "            sentence_sequences.append(tokenized[:i])\n",
        "\n",
        "print(f\"Generated {len(sliding_sequences):,} sliding window sequences\")\n",
        "print(f\"Generated {len(sentence_sequences):,} sentence-based sequences\")\n",
        "\n",
        "# Combine both approaches for richer training data\n",
        "all_word_sequences = sliding_sequences + sentence_sequences\n",
        "\n",
        "# Convert to indices\n",
        "sequences = []\n",
        "for seq in all_word_sequences:\n",
        "    tokenized = [word2idx.get(w, word2idx['<UNK>']) for w in seq]\n",
        "    sequences.append(tokenized)\n",
        "\n",
        "print(f\"Total sequences: {len(sequences):,}\")\n",
        "print(\"Sample sequences (first 5):\")\n",
        "for i, seq in enumerate(sequences[:5]):\n",
        "    words = [idx2word[idx] for idx in seq]\n",
        "    print(f\"  {i+1}: {' '.join(words)}\")\n",
        "\n",
        "max_seq_len = min(50, max(len(seq) for seq in sequences))\n",
        "sequences = [([0] * (max_seq_len - len(seq)) + seq)[-max_seq_len:] for seq in sequences]\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = torch.tensor(y).long()\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X).long()\n",
        "        self.y = torch.tensor(y).long() if not isinstance(y, torch.Tensor) else y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(TextDataset(X_val, y_val), batch_size=64)\n",
        "test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=64)\n",
        "\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, attention_units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention_dense = nn.Linear(hidden_dim, attention_units)\n",
        "        self.context_vector = nn.Linear(attention_units, 1, bias=False)\n",
        "\n",
        "    def forward(self, lstm_out):\n",
        "        score = torch.tanh(self.attention_dense(lstm_out))\n",
        "        attention_weights = F.softmax(self.context_vector(score), dim=1)\n",
        "        context_vector = attention_weights * lstm_out\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        return context_vector, attention_weights.squeeze(-1)\n",
        "\n",
        "class LSTMAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, lstm_units=100, attention_units=128):\n",
        "        super(LSTMAttentionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, lstm_units, batch_first=True, dropout=0.2)\n",
        "        self.attention = AttentionLayer(lstm_units, attention_units)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(lstm_units, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        context, attention_weights = self.attention(lstm_out)\n",
        "        out = self.dropout(context)\n",
        "        return self.fc(out), attention_weights\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMAttentionModel(len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(inputs)\n",
        "        loss = criterion(outputs, labels)  # labels are now integer indices, not one-hot\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "        total += inputs.size(0)\n",
        "\n",
        "    train_loss = total_loss / total\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)  # labels are now integer indices, not one-hot\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "            val_total += inputs.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, data_loader, device):\n",
        "    \"\"\"Calculate perplexity on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    perplexity = np.exp(avg_loss)\n",
        "    return perplexity\n",
        "\n",
        "def generate_text(model, start_text, max_length=50, temperature=1.0):\n",
        "    \"\"\"Generate text using the trained model, stopping at EOS token\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize start text\n",
        "    tokens = start_text.lower().split()\n",
        "    sequence = [start_idx]  # Start with START token\n",
        "\n",
        "    # Add start text tokens\n",
        "    for token in tokens:\n",
        "        if token in word2idx:\n",
        "            sequence.append(word2idx[token])\n",
        "        else:\n",
        "            sequence.append(word2idx['<UNK>'])\n",
        "\n",
        "    generated_tokens = tokens.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Prepare input sequence\n",
        "            input_seq = torch.tensor([sequence[-max_seq_len+1:]]).to(device)\n",
        "            if len(sequence) < max_seq_len - 1:\n",
        "                # Pad if sequence is shorter than expected\n",
        "                padding = [0] * (max_seq_len - 1 - len(sequence))\n",
        "                input_seq = torch.tensor([padding + sequence]).to(device)\n",
        "\n",
        "            # Get prediction\n",
        "            output, _ = model(input_seq)\n",
        "\n",
        "            # Apply temperature sampling\n",
        "            logits = output[0] / temperature\n",
        "            probabilities = torch.softmax(logits, dim=0)\n",
        "\n",
        "            # Sample next token\n",
        "            next_token_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token_idx == eos_idx:\n",
        "                break\n",
        "\n",
        "            # Add token to sequence\n",
        "            sequence.append(next_token_idx)\n",
        "\n",
        "            # Convert to word and add to generated text\n",
        "            if next_token_idx in idx2word:\n",
        "                word = idx2word[next_token_idx]\n",
        "                if word not in ['<PAD>', '<UNK>', START_TOKEN]:\n",
        "                    generated_tokens.append(word)\n",
        "\n",
        "    return ' '.join(generated_tokens)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📊 MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Calculate perplexity for all datasets\n",
        "train_perplexity = calculate_perplexity(model, train_loader, device)\n",
        "val_perplexity = calculate_perplexity(model, val_loader, device)\n",
        "test_perplexity = calculate_perplexity(model, test_loader, device)\n",
        "\n",
        "print(f\"🎯 PERPLEXITY SCORES:\")\n",
        "print(f\"  📈 Training Perplexity: {train_perplexity:.2f}\")\n",
        "print(f\"  📈 Validation Perplexity: {val_perplexity:.2f}\")\n",
        "print(f\"  📈 Test Perplexity: {test_perplexity:.2f}\")\n",
        "\n",
        "print(f\"\\n🔮 TEXT GENERATION EXAMPLES:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test text generation\n",
        "test_prompts = [\n",
        "    \"Holmes said\",\n",
        "    \"Watson was\",\n",
        "    \"The detective\",\n",
        "    \"I saw\",\n",
        "    \"It was a dark\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(model, prompt, max_length=20, temperature=0.8)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: '{generated}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "print(f\"\\n✅ SUMMARY:\")\n",
        "print(f\"  🎯 Best Test Perplexity: {test_perplexity:.2f}\")\n",
        "print(f\"  🔤 Vocabulary Size: {len(vocab):,}\")\n",
        "print(f\"  🏁 START/EOS tokens: Implemented\")\n",
        "print(f\"  📊 Label Encoding: Used (instead of one-hot)\")\n",
        "print(\"=\"*50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n05nArgLLWHU",
        "outputId": "0f3f1b38-3d99-4a5f-d823-7ee3f4645117"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Using cached file: sherlock_holmes.txt\n",
            "🔧 Creating vocabulary from tokens...\n",
            "START token index: 2, EOS token index: 3\n",
            "🔧 Generating sequences with cross-sentence context...\n",
            "Generated 104,462 sliding window sequences\n",
            "Generated 113,459 sentence-based sequences\n",
            "Total sequences: 217,921\n",
            "Sample sequences (first 5):\n",
            "  1: start of the project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock\n",
            "  2: of the project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes\n",
            "  3: the project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes by\n",
            "  4: project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes by arthur\n",
            "  5: gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes by arthur conan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1 | Train Loss: 6.7340 | Train Acc: 0.0794 | Val Loss: 6.3409 | Val Acc: 0.1089\n",
            "Epoch  2 | Train Loss: 6.0277 | Train Acc: 0.1281 | Val Loss: 5.9546 | Val Acc: 0.1413\n",
            "Epoch  3 | Train Loss: 5.5935 | Train Acc: 0.1515 | Val Loss: 5.7317 | Val Acc: 0.1543\n",
            "Epoch  4 | Train Loss: 5.2343 | Train Acc: 0.1672 | Val Loss: 5.5857 | Val Acc: 0.1665\n",
            "Epoch  5 | Train Loss: 4.9163 | Train Acc: 0.1814 | Val Loss: 5.4564 | Val Acc: 0.1754\n",
            "Epoch  6 | Train Loss: 4.6337 | Train Acc: 0.1979 | Val Loss: 5.3674 | Val Acc: 0.1798\n",
            "Epoch  7 | Train Loss: 4.3812 | Train Acc: 0.2162 | Val Loss: 5.2944 | Val Acc: 0.1866\n",
            "Epoch  8 | Train Loss: 4.1515 | Train Acc: 0.2372 | Val Loss: 5.2139 | Val Acc: 0.1913\n",
            "Epoch  9 | Train Loss: 3.9431 | Train Acc: 0.2582 | Val Loss: 5.1553 | Val Acc: 0.1977\n",
            "Epoch 10 | Train Loss: 3.7612 | Train Acc: 0.2792 | Val Loss: 5.1113 | Val Acc: 0.2028\n",
            "Epoch 11 | Train Loss: 3.5949 | Train Acc: 0.3000 | Val Loss: 5.0390 | Val Acc: 0.2102\n",
            "Epoch 12 | Train Loss: 3.4452 | Train Acc: 0.3198 | Val Loss: 4.9749 | Val Acc: 0.2176\n",
            "Epoch 13 | Train Loss: 3.3117 | Train Acc: 0.3395 | Val Loss: 4.9286 | Val Acc: 0.2238\n",
            "Epoch 14 | Train Loss: 3.1885 | Train Acc: 0.3573 | Val Loss: 4.8865 | Val Acc: 0.2311\n",
            "Epoch 15 | Train Loss: 3.0748 | Train Acc: 0.3745 | Val Loss: 4.8507 | Val Acc: 0.2387\n",
            "Epoch 16 | Train Loss: 2.9715 | Train Acc: 0.3913 | Val Loss: 4.8422 | Val Acc: 0.2423\n",
            "Epoch 17 | Train Loss: 2.8756 | Train Acc: 0.4039 | Val Loss: 4.7879 | Val Acc: 0.2506\n",
            "Epoch 18 | Train Loss: 2.7920 | Train Acc: 0.4196 | Val Loss: 4.7459 | Val Acc: 0.2589\n",
            "Epoch 19 | Train Loss: 2.7063 | Train Acc: 0.4326 | Val Loss: 4.7305 | Val Acc: 0.2638\n",
            "Epoch 20 | Train Loss: 2.6440 | Train Acc: 0.4431 | Val Loss: 4.7063 | Val Acc: 0.2693\n",
            "Epoch 21 | Train Loss: 2.5717 | Train Acc: 0.4545 | Val Loss: 4.6881 | Val Acc: 0.2752\n",
            "Epoch 22 | Train Loss: 2.5062 | Train Acc: 0.4662 | Val Loss: 4.6669 | Val Acc: 0.2797\n",
            "Epoch 23 | Train Loss: 2.4594 | Train Acc: 0.4737 | Val Loss: 4.6577 | Val Acc: 0.2866\n",
            "Epoch 24 | Train Loss: 2.3938 | Train Acc: 0.4855 | Val Loss: 4.6499 | Val Acc: 0.2893\n",
            "Epoch 25 | Train Loss: 2.3476 | Train Acc: 0.4938 | Val Loss: 4.6408 | Val Acc: 0.2934\n",
            "Epoch 26 | Train Loss: 2.2987 | Train Acc: 0.5024 | Val Loss: 4.6181 | Val Acc: 0.2983\n",
            "Epoch 27 | Train Loss: 2.2488 | Train Acc: 0.5119 | Val Loss: 4.5995 | Val Acc: 0.3020\n",
            "Epoch 28 | Train Loss: 2.2244 | Train Acc: 0.5160 | Val Loss: 4.6024 | Val Acc: 0.3033\n",
            "Epoch 29 | Train Loss: 2.1771 | Train Acc: 0.5242 | Val Loss: 4.6023 | Val Acc: 0.3064\n",
            "Epoch 30 | Train Loss: 2.1403 | Train Acc: 0.5302 | Val Loss: 4.5969 | Val Acc: 0.3130\n",
            "Epoch 31 | Train Loss: 2.1069 | Train Acc: 0.5364 | Val Loss: 4.5907 | Val Acc: 0.3130\n",
            "Epoch 32 | Train Loss: 2.0652 | Train Acc: 0.5449 | Val Loss: 4.5957 | Val Acc: 0.3147\n",
            "Epoch 33 | Train Loss: 2.0477 | Train Acc: 0.5482 | Val Loss: 4.6087 | Val Acc: 0.3154\n",
            "Epoch 34 | Train Loss: 2.0144 | Train Acc: 0.5536 | Val Loss: 4.5839 | Val Acc: 0.3213\n",
            "Epoch 35 | Train Loss: 1.9949 | Train Acc: 0.5588 | Val Loss: 4.5869 | Val Acc: 0.3214\n",
            "Epoch 36 | Train Loss: 1.9591 | Train Acc: 0.5652 | Val Loss: 4.5870 | Val Acc: 0.3257\n",
            "Epoch 37 | Train Loss: 1.9376 | Train Acc: 0.5691 | Val Loss: 4.6005 | Val Acc: 0.3239\n",
            "Epoch 38 | Train Loss: 1.9169 | Train Acc: 0.5739 | Val Loss: 4.5914 | Val Acc: 0.3281\n",
            "Epoch 39 | Train Loss: 1.8952 | Train Acc: 0.5779 | Val Loss: 4.6003 | Val Acc: 0.3262\n",
            "Epoch 40 | Train Loss: 1.8718 | Train Acc: 0.5805 | Val Loss: 4.5766 | Val Acc: 0.3352\n",
            "Epoch 41 | Train Loss: 1.8545 | Train Acc: 0.5856 | Val Loss: 4.5851 | Val Acc: 0.3328\n",
            "Epoch 42 | Train Loss: 1.8322 | Train Acc: 0.5886 | Val Loss: 4.5830 | Val Acc: 0.3353\n",
            "Epoch 43 | Train Loss: 1.8185 | Train Acc: 0.5909 | Val Loss: 4.5871 | Val Acc: 0.3346\n",
            "Epoch 44 | Train Loss: 1.7996 | Train Acc: 0.5949 | Val Loss: 4.5911 | Val Acc: 0.3383\n",
            "Epoch 45 | Train Loss: 1.7799 | Train Acc: 0.5978 | Val Loss: 4.6026 | Val Acc: 0.3351\n",
            "Epoch 46 | Train Loss: 1.7591 | Train Acc: 0.6018 | Val Loss: 4.6062 | Val Acc: 0.3358\n",
            "Epoch 47 | Train Loss: 1.7585 | Train Acc: 0.6026 | Val Loss: 4.5893 | Val Acc: 0.3397\n",
            "Epoch 48 | Train Loss: 1.7368 | Train Acc: 0.6070 | Val Loss: 4.5995 | Val Acc: 0.3406\n",
            "Epoch 49 | Train Loss: 1.7270 | Train Acc: 0.6085 | Val Loss: 4.6121 | Val Acc: 0.3403\n",
            "Epoch 50 | Train Loss: 1.6997 | Train Acc: 0.6141 | Val Loss: 4.6247 | Val Acc: 0.3374\n",
            "Epoch 51 | Train Loss: 1.6973 | Train Acc: 0.6144 | Val Loss: 4.6277 | Val Acc: 0.3397\n",
            "Epoch 52 | Train Loss: 1.6889 | Train Acc: 0.6149 | Val Loss: 4.6016 | Val Acc: 0.3453\n",
            "Epoch 53 | Train Loss: 1.6778 | Train Acc: 0.6198 | Val Loss: 4.6410 | Val Acc: 0.3439\n",
            "Epoch 54 | Train Loss: 1.6587 | Train Acc: 0.6219 | Val Loss: 4.6369 | Val Acc: 0.3435\n",
            "Epoch 55 | Train Loss: 1.6545 | Train Acc: 0.6239 | Val Loss: 4.6279 | Val Acc: 0.3451\n",
            "Epoch 56 | Train Loss: 1.6495 | Train Acc: 0.6237 | Val Loss: 4.6244 | Val Acc: 0.3476\n",
            "Epoch 57 | Train Loss: 1.6224 | Train Acc: 0.6296 | Val Loss: 4.6447 | Val Acc: 0.3454\n",
            "Epoch 58 | Train Loss: 1.6290 | Train Acc: 0.6281 | Val Loss: 4.6308 | Val Acc: 0.3465\n",
            "Epoch 59 | Train Loss: 1.6104 | Train Acc: 0.6321 | Val Loss: 4.6471 | Val Acc: 0.3460\n",
            "Epoch 60 | Train Loss: 1.5999 | Train Acc: 0.6347 | Val Loss: 4.6486 | Val Acc: 0.3460\n",
            "Epoch 61 | Train Loss: 1.5930 | Train Acc: 0.6358 | Val Loss: 4.6650 | Val Acc: 0.3488\n",
            "Epoch 62 | Train Loss: 1.5802 | Train Acc: 0.6376 | Val Loss: 4.6687 | Val Acc: 0.3471\n",
            "Epoch 63 | Train Loss: 1.5844 | Train Acc: 0.6359 | Val Loss: 4.6585 | Val Acc: 0.3488\n",
            "Epoch 64 | Train Loss: 1.5626 | Train Acc: 0.6415 | Val Loss: 4.6620 | Val Acc: 0.3519\n",
            "Epoch 65 | Train Loss: 1.5669 | Train Acc: 0.6393 | Val Loss: 4.6802 | Val Acc: 0.3488\n",
            "Epoch 66 | Train Loss: 1.5532 | Train Acc: 0.6427 | Val Loss: 4.6683 | Val Acc: 0.3509\n",
            "Epoch 67 | Train Loss: 1.5592 | Train Acc: 0.6421 | Val Loss: 4.6583 | Val Acc: 0.3507\n",
            "Epoch 68 | Train Loss: 1.5394 | Train Acc: 0.6455 | Val Loss: 4.6832 | Val Acc: 0.3506\n",
            "Epoch 69 | Train Loss: 1.5232 | Train Acc: 0.6496 | Val Loss: 4.6762 | Val Acc: 0.3516\n",
            "Epoch 70 | Train Loss: 1.5314 | Train Acc: 0.6468 | Val Loss: 4.6783 | Val Acc: 0.3500\n",
            "Epoch 71 | Train Loss: 1.5310 | Train Acc: 0.6480 | Val Loss: 4.6725 | Val Acc: 0.3546\n",
            "Epoch 72 | Train Loss: 1.5185 | Train Acc: 0.6512 | Val Loss: 4.6894 | Val Acc: 0.3519\n",
            "Epoch 73 | Train Loss: 1.5142 | Train Acc: 0.6499 | Val Loss: 4.6934 | Val Acc: 0.3494\n",
            "Epoch 74 | Train Loss: 1.4987 | Train Acc: 0.6548 | Val Loss: 4.7048 | Val Acc: 0.3487\n",
            "Epoch 75 | Train Loss: 1.4949 | Train Acc: 0.6555 | Val Loss: 4.6884 | Val Acc: 0.3511\n",
            "Epoch 76 | Train Loss: 1.4872 | Train Acc: 0.6566 | Val Loss: 4.7133 | Val Acc: 0.3536\n",
            "Epoch 77 | Train Loss: 1.4925 | Train Acc: 0.6558 | Val Loss: 4.7020 | Val Acc: 0.3539\n",
            "Epoch 78 | Train Loss: 1.4785 | Train Acc: 0.6586 | Val Loss: 4.7004 | Val Acc: 0.3542\n",
            "Epoch 79 | Train Loss: 1.4804 | Train Acc: 0.6573 | Val Loss: 4.7068 | Val Acc: 0.3540\n",
            "Epoch 80 | Train Loss: 1.4596 | Train Acc: 0.6629 | Val Loss: 4.7414 | Val Acc: 0.3495\n",
            "Epoch 81 | Train Loss: 1.4735 | Train Acc: 0.6597 | Val Loss: 4.7726 | Val Acc: 0.3464\n",
            "Epoch 82 | Train Loss: 1.4805 | Train Acc: 0.6561 | Val Loss: 4.7181 | Val Acc: 0.3534\n",
            "Epoch 83 | Train Loss: 1.4653 | Train Acc: 0.6625 | Val Loss: 4.7446 | Val Acc: 0.3525\n",
            "Epoch 84 | Train Loss: 1.4604 | Train Acc: 0.6614 | Val Loss: 4.7369 | Val Acc: 0.3533\n",
            "Epoch 85 | Train Loss: 1.4473 | Train Acc: 0.6642 | Val Loss: 4.7256 | Val Acc: 0.3555\n",
            "Epoch 86 | Train Loss: 1.4341 | Train Acc: 0.6685 | Val Loss: 4.7297 | Val Acc: 0.3545\n",
            "Epoch 87 | Train Loss: 1.4450 | Train Acc: 0.6646 | Val Loss: 4.7388 | Val Acc: 0.3575\n",
            "Epoch 88 | Train Loss: 1.4346 | Train Acc: 0.6680 | Val Loss: 4.7368 | Val Acc: 0.3559\n",
            "Epoch 89 | Train Loss: 1.4354 | Train Acc: 0.6665 | Val Loss: 4.7329 | Val Acc: 0.3571\n",
            "Epoch 90 | Train Loss: 1.4338 | Train Acc: 0.6690 | Val Loss: 4.7483 | Val Acc: 0.3538\n",
            "Epoch 91 | Train Loss: 1.4240 | Train Acc: 0.6688 | Val Loss: 4.7551 | Val Acc: 0.3554\n",
            "Epoch 92 | Train Loss: 1.4268 | Train Acc: 0.6689 | Val Loss: 4.7340 | Val Acc: 0.3588\n",
            "Epoch 93 | Train Loss: 1.4284 | Train Acc: 0.6683 | Val Loss: 4.7577 | Val Acc: 0.3560\n",
            "Epoch 94 | Train Loss: 1.4176 | Train Acc: 0.6714 | Val Loss: 4.7527 | Val Acc: 0.3564\n",
            "Epoch 95 | Train Loss: 1.4171 | Train Acc: 0.6700 | Val Loss: 4.7499 | Val Acc: 0.3574\n",
            "Epoch 96 | Train Loss: 1.4022 | Train Acc: 0.6746 | Val Loss: 4.7651 | Val Acc: 0.3573\n",
            "Epoch 97 | Train Loss: 1.4095 | Train Acc: 0.6711 | Val Loss: 4.7693 | Val Acc: 0.3569\n",
            "Epoch 98 | Train Loss: 1.4034 | Train Acc: 0.6736 | Val Loss: 4.7637 | Val Acc: 0.3591\n",
            "Epoch 99 | Train Loss: 1.4029 | Train Acc: 0.6744 | Val Loss: 4.7804 | Val Acc: 0.3560\n",
            "Epoch 100 | Train Loss: 1.4028 | Train Acc: 0.6732 | Val Loss: 4.7657 | Val Acc: 0.3571\n",
            "✅ Model saved as 'sherlock_lstm_attention_pytorch.pth'\n",
            "\n",
            "==================================================\n",
            "📊 MODEL EVALUATION\n",
            "==================================================\n",
            "🎯 PERPLEXITY SCORES:\n",
            "  📈 Training Perplexity: 2.22\n",
            "  📈 Validation Perplexity: 117.41\n",
            "  📈 Test Perplexity: 119.94\n",
            "\n",
            "🔮 TEXT GENERATION EXAMPLES:\n",
            "----------------------------------------\n",
            "Prompt: 'Holmes said'\n",
            "Generated: 'holmes said that he had a few minutes after the king of bohemia, and a woman in his chair and never had'\n",
            "----------------------------------------\n",
            "Prompt: 'Watson was'\n",
            "Generated: 'watson was a very deep child, and'\n",
            "----------------------------------------\n",
            "Prompt: 'The detective'\n",
            "Generated: 'the detective had come. where the door lady and you had cut cut the intimate with the most perfect of age, be'\n",
            "----------------------------------------\n",
            "Prompt: 'I saw'\n",
            "Generated: 'i saw that the impression of the man, however, and was hot upon the girl's short at work.'\n",
            "----------------------------------------\n",
            "Prompt: 'It was a dark'\n",
            "Generated: 'it was a dark bent with a despairing gesture, in the witness. it was a human startled life without discovered which you had recovered'\n",
            "----------------------------------------\n",
            "\n",
            "✅ SUMMARY:\n",
            "  🎯 Best Test Perplexity: 119.94\n",
            "  🔤 Vocabulary Size: 14,085\n",
            "  🏁 START/EOS tokens: Implemented\n",
            "  📊 Label Encoding: Used (instead of one-hot)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As we can see from this experiment, even though initially the validation loss had began to lower, it soon started oscillating and ended up rising. This is mostly due to overfiting. The model maybe too complex to learn from such a small corpus.**"
      ],
      "metadata": {
        "id": "yAIXgaF73dgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacked LSTM with Attention"
      ],
      "metadata": {
        "id": "sN81P70V2S2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Download & Preprocess Text\n",
        "# -----------------------------\n",
        "\n",
        "def download_sherlock_holmes():\n",
        "    url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
        "    filename = \"sherlock_holmes.txt\"\n",
        "\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"📥 Downloading Sherlock Holmes text...\")\n",
        "        r = requests.get(url, timeout=30)\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(r.text)\n",
        "        print(\"✅ Downloaded and saved as\", filename)\n",
        "    else:\n",
        "        print(\"✓ Using cached file:\", filename)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[\"“”]', '\"', text)\n",
        "    text = re.sub(r\"[’‘']\", \"'\", text)\n",
        "    text = re.sub(r'[—–]', '-', text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\;\\:\\'\\\"\\-\\(\\)]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "download_sherlock_holmes()\n",
        "\n",
        "with open('sherlock_holmes.txt', 'r', encoding='utf-8') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "start_idx = raw_text.find(\"*** START OF\")\n",
        "end_idx = raw_text.find(\"*** END OF\")\n",
        "text = raw_text[start_idx:end_idx] if start_idx != -1 and end_idx != -1 else raw_text\n",
        "text = clean_text(text)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Tokenization & Sequence Creation\n",
        "# -----------------------------\n",
        "\n",
        "import re\n",
        "\n",
        "# Add special tokens for sequence control\n",
        "START_TOKEN = '<START>'\n",
        "EOS_TOKEN = '<EOS>'\n",
        "\n",
        "def proper_sentence_split(text):\n",
        "    \"\"\"Better sentence splitting that handles punctuation correctly\"\"\"\n",
        "    # Split on sentence-ending punctuation, but keep the punctuation\n",
        "    sentences = re.split(r'([.!?]+)', text)\n",
        "\n",
        "    # Recombine sentences with their punctuation\n",
        "    result = []\n",
        "    for i in range(0, len(sentences) - 1, 2):\n",
        "        sentence = sentences[i].strip()\n",
        "        punct = sentences[i + 1] if i + 1 < len(sentences) else ''\n",
        "        if sentence:  # Only add non-empty sentences\n",
        "            result.append(sentence + punct)\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_sliding_window_sequences(text, window_size=10):\n",
        "    \"\"\"Create overlapping sequences using sliding window approach\"\"\"\n",
        "    tokens = text.split()\n",
        "    sequences = []\n",
        "\n",
        "    # Create overlapping sequences across the entire text\n",
        "    for i in range(len(tokens) - window_size + 1):\n",
        "        sequence = tokens[i:i + window_size]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "print(\"🔧 Creating vocabulary from tokens...\")\n",
        "tokens = text.split()\n",
        "vocab = ['<PAD>', '<UNK>', START_TOKEN, EOS_TOKEN] + sorted(set(tokens))\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx2word = {i: word for word, i in word2idx.items()}\n",
        "\n",
        "# Get indices for special tokens\n",
        "start_idx = word2idx[START_TOKEN]\n",
        "eos_idx = word2idx[EOS_TOKEN]\n",
        "print(f\"START token index: {start_idx}, EOS token index: {eos_idx}\")\n",
        "\n",
        "print(\"🔧 Generating sequences with cross-sentence context...\")\n",
        "\n",
        "# Method 1: Sliding window across entire text (for cross-sentence learning)\n",
        "sliding_sequences = create_sliding_window_sequences(text, window_size=15)\n",
        "\n",
        "# Method 2: Proper sentence-based sequences with START/EOS tokens\n",
        "sentences = proper_sentence_split(text)\n",
        "sentence_sequences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence_words = sentence.strip().split()\n",
        "    if len(sentence_words) >= 2:  # Only process sentences with at least 2 words\n",
        "        # Add START and EOS tokens\n",
        "        tokenized = [START_TOKEN] + sentence_words + [EOS_TOKEN]\n",
        "        # Create progressive sequences\n",
        "        for i in range(2, len(tokenized) + 1):\n",
        "            sentence_sequences.append(tokenized[:i])\n",
        "\n",
        "print(f\"Generated {len(sliding_sequences):,} sliding window sequences\")\n",
        "print(f\"Generated {len(sentence_sequences):,} sentence-based sequences\")\n",
        "\n",
        "# Combine both approaches for richer training data\n",
        "all_word_sequences = sliding_sequences + sentence_sequences\n",
        "\n",
        "# Convert to indices\n",
        "sequences = []\n",
        "for seq in all_word_sequences:\n",
        "    tokenized = [word2idx.get(w, word2idx['<UNK>']) for w in seq]\n",
        "    sequences.append(tokenized)\n",
        "\n",
        "print(f\"Total sequences: {len(sequences):,}\")\n",
        "print(\"Sample sequences (first 5):\")\n",
        "for i, seq in enumerate(sequences[:5]):\n",
        "    words = [idx2word[idx] for idx in seq]\n",
        "    print(f\"  {i+1}: {' '.join(words)}\")\n",
        "\n",
        "max_seq_len = min(50, max(len(seq) for seq in sequences))\n",
        "sequences = [([0] * (max_seq_len - len(seq)) + seq)[-max_seq_len:] for seq in sequences]\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "# Keep y as label encoding (integer labels) instead of one-hot encoding\n",
        "y = torch.tensor(y).long()\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1765, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Dataset & DataLoader\n",
        "# -----------------------------\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X).long()\n",
        "        self.y = torch.tensor(y).long() if not isinstance(y, torch.Tensor) else y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(TextDataset(X_val, y_val), batch_size=64)\n",
        "test_loader = DataLoader(TextDataset(X_test, y_test), batch_size=64)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Model Definition\n",
        "# -----------------------------\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, attention_units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention_dense = nn.Linear(hidden_dim, attention_units)\n",
        "        self.context_vector = nn.Linear(attention_units, 1, bias=False)\n",
        "\n",
        "    def forward(self, lstm_out):\n",
        "        score = torch.tanh(self.attention_dense(lstm_out))\n",
        "        attention_weights = torch.softmax(self.context_vector(score), dim=1)\n",
        "        context_vector = attention_weights * lstm_out\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        return context_vector, attention_weights.squeeze(-1)\n",
        "\n",
        "class LSTMAttentionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, lstm_units=100, attention_units=64):\n",
        "        super(LSTMAttentionModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, lstm_units, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_units, lstm_units, batch_first=True)\n",
        "        self.attention = AttentionLayer(lstm_units, attention_units)\n",
        "        self.layer_norm = nn.LayerNorm(lstm_units)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(lstm_units, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.layer_norm(x)\n",
        "        context, attention_weights = self.attention(x)\n",
        "        out = self.dropout(context)\n",
        "        return self.fc(out), attention_weights\n",
        "# -----------------------------\n",
        "# 5. Training Loop\n",
        "# -----------------------------\n",
        "\n",
        "import torch.nn.utils as nn_utils\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMAttentionModel(len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=True,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping: clip gradients to max norm 1.0\n",
        "        nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "        total += inputs.size(0)\n",
        "\n",
        "    train_loss = total_loss / total\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_correct += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "            val_total += inputs.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Step scheduler with validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "\n",
        "# Save Model\n",
        "torch.save(model.state_dict(), \"sherlock_lstm_attention_pytorch.pth\")\n",
        "print(\"Model saved as 'sherlock_lstm_attention_pytorch_stacked.pth'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V2obdoQRNdB",
        "outputId": "7cfb9b68-b545-4441-e14b-66d4157d073b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Using cached file: sherlock_holmes.txt\n",
            "🔧 Creating vocabulary from tokens...\n",
            "START token index: 2, EOS token index: 3\n",
            "🔧 Generating sequences with cross-sentence context...\n",
            "Generated 104,462 sliding window sequences\n",
            "Generated 113,459 sentence-based sequences\n",
            "Total sequences: 217,921\n",
            "Sample sequences (first 5):\n",
            "  1: start of the project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock\n",
            "  2: of the project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes\n",
            "  3: the project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes by\n",
            "  4: project gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes by arthur\n",
            "  5: gutenberg ebook the adventures of sherlock holmes the adventures of sherlock holmes by arthur conan\n",
            "Epoch  1 | Train Loss: 7.1975 | Train Acc: 0.0683 | Val Loss: 7.0215 | Val Acc: 0.0733\n",
            "Epoch  2 | Train Loss: 6.7649 | Train Acc: 0.1010 | Val Loss: 6.5504 | Val Acc: 0.1258\n",
            "Epoch  3 | Train Loss: 6.3528 | Train Acc: 0.1391 | Val Loss: 6.2887 | Val Acc: 0.1463\n",
            "Epoch  4 | Train Loss: 6.0543 | Train Acc: 0.1632 | Val Loss: 6.1267 | Val Acc: 0.1612\n",
            "Epoch  5 | Train Loss: 5.8180 | Train Acc: 0.1783 | Val Loss: 6.0103 | Val Acc: 0.1688\n",
            "Epoch  6 | Train Loss: 5.6076 | Train Acc: 0.1928 | Val Loss: 5.9187 | Val Acc: 0.1758\n",
            "Epoch  7 | Train Loss: 5.4244 | Train Acc: 0.2037 | Val Loss: 5.8339 | Val Acc: 0.1849\n",
            "Epoch  8 | Train Loss: 5.2692 | Train Acc: 0.2158 | Val Loss: 5.7851 | Val Acc: 0.1877\n",
            "Epoch  9 | Train Loss: 5.1492 | Train Acc: 0.2276 | Val Loss: 5.7408 | Val Acc: 0.1888\n",
            "Epoch 10 | Train Loss: 5.0359 | Train Acc: 0.2394 | Val Loss: 5.7395 | Val Acc: 0.1928\n",
            "Epoch 11 | Train Loss: 4.9376 | Train Acc: 0.2500 | Val Loss: 5.6534 | Val Acc: 0.1948\n",
            "Epoch 12 | Train Loss: 4.8497 | Train Acc: 0.2618 | Val Loss: 5.6521 | Val Acc: 0.1987\n",
            "Epoch 13 | Train Loss: 4.8375 | Train Acc: 0.2661 | Val Loss: 5.6117 | Val Acc: 0.1999\n",
            "Epoch 14 | Train Loss: 4.7642 | Train Acc: 0.2735 | Val Loss: 5.6021 | Val Acc: 0.2020\n",
            "Epoch 15 | Train Loss: 4.6938 | Train Acc: 0.2830 | Val Loss: 5.5916 | Val Acc: 0.2042\n",
            "Epoch 16 | Train Loss: 4.6534 | Train Acc: 0.2891 | Val Loss: 5.5508 | Val Acc: 0.2060\n",
            "Epoch 17 | Train Loss: 4.6123 | Train Acc: 0.2942 | Val Loss: 5.5325 | Val Acc: 0.2086\n",
            "Epoch 18 | Train Loss: 4.5839 | Train Acc: 0.2981 | Val Loss: 5.5375 | Val Acc: 0.2103\n",
            "Epoch 19 | Train Loss: 4.5557 | Train Acc: 0.3026 | Val Loss: 5.5239 | Val Acc: 0.2117\n",
            "Epoch 20 | Train Loss: 4.5370 | Train Acc: 0.3053 | Val Loss: 5.5136 | Val Acc: 0.2138\n",
            "Epoch 21 | Train Loss: 4.5363 | Train Acc: 0.3065 | Val Loss: 5.5031 | Val Acc: 0.2122\n",
            "Epoch 22 | Train Loss: 4.5168 | Train Acc: 0.3082 | Val Loss: 5.5155 | Val Acc: 0.2124\n",
            "Epoch 23 | Train Loss: 4.5012 | Train Acc: 0.3095 | Val Loss: 5.4871 | Val Acc: 0.2118\n",
            "Epoch 24 | Train Loss: 4.4945 | Train Acc: 0.3114 | Val Loss: 5.4818 | Val Acc: 0.2166\n",
            "Epoch 25 | Train Loss: 4.4811 | Train Acc: 0.3121 | Val Loss: 5.4755 | Val Acc: 0.2139\n",
            "Epoch 26 | Train Loss: 4.4590 | Train Acc: 0.3159 | Val Loss: 5.4918 | Val Acc: 0.2161\n",
            "Epoch 27 | Train Loss: 4.5000 | Train Acc: 0.3156 | Val Loss: 5.5854 | Val Acc: 0.2038\n",
            "Epoch 28 | Train Loss: 4.5132 | Train Acc: 0.3095 | Val Loss: 5.4829 | Val Acc: 0.2131\n",
            "Epoch 29 | Train Loss: 4.4734 | Train Acc: 0.3132 | Val Loss: 5.4647 | Val Acc: 0.2172\n",
            "Epoch 30 | Train Loss: 4.4584 | Train Acc: 0.3157 | Val Loss: 5.4740 | Val Acc: 0.2146\n",
            "Epoch 31 | Train Loss: 4.4688 | Train Acc: 0.3136 | Val Loss: 5.4710 | Val Acc: 0.2136\n",
            "Epoch 32 | Train Loss: 4.4450 | Train Acc: 0.3166 | Val Loss: 5.4677 | Val Acc: 0.2173\n",
            "Epoch 33 | Train Loss: 4.4426 | Train Acc: 0.3170 | Val Loss: 5.4642 | Val Acc: 0.2162\n",
            "Epoch 34 | Train Loss: 4.1702 | Train Acc: 0.3641 | Val Loss: 5.3778 | Val Acc: 0.2333\n",
            "Epoch 35 | Train Loss: 4.0519 | Train Acc: 0.3868 | Val Loss: 5.3413 | Val Acc: 0.2430\n",
            "Epoch 36 | Train Loss: 4.0016 | Train Acc: 0.3966 | Val Loss: 5.3122 | Val Acc: 0.2433\n",
            "Epoch 37 | Train Loss: 3.9674 | Train Acc: 0.4007 | Val Loss: 5.3197 | Val Acc: 0.2494\n",
            "Epoch 38 | Train Loss: 3.9389 | Train Acc: 0.4076 | Val Loss: 5.3075 | Val Acc: 0.2463\n",
            "Epoch 39 | Train Loss: 3.9211 | Train Acc: 0.4105 | Val Loss: 5.2892 | Val Acc: 0.2541\n",
            "Epoch 40 | Train Loss: 3.9181 | Train Acc: 0.4122 | Val Loss: 5.2961 | Val Acc: 0.2530\n",
            "Epoch 41 | Train Loss: 3.9038 | Train Acc: 0.4129 | Val Loss: 5.3057 | Val Acc: 0.2550\n",
            "Epoch 42 | Train Loss: 3.8871 | Train Acc: 0.4173 | Val Loss: 5.2889 | Val Acc: 0.2543\n",
            "Epoch 43 | Train Loss: 3.8722 | Train Acc: 0.4182 | Val Loss: 5.2911 | Val Acc: 0.2599\n",
            "Epoch 44 | Train Loss: 3.7024 | Train Acc: 0.4553 | Val Loss: 5.2358 | Val Acc: 0.2690\n",
            "Epoch 45 | Train Loss: 3.6357 | Train Acc: 0.4707 | Val Loss: 5.2324 | Val Acc: 0.2726\n",
            "Epoch 46 | Train Loss: 3.6120 | Train Acc: 0.4729 | Val Loss: 5.2356 | Val Acc: 0.2755\n",
            "Epoch 47 | Train Loss: 3.5909 | Train Acc: 0.4765 | Val Loss: 5.2211 | Val Acc: 0.2792\n",
            "Epoch 48 | Train Loss: 3.5792 | Train Acc: 0.4793 | Val Loss: 5.2190 | Val Acc: 0.2786\n",
            "Epoch 49 | Train Loss: 3.5658 | Train Acc: 0.4816 | Val Loss: 5.2224 | Val Acc: 0.2779\n",
            "Epoch 50 | Train Loss: 3.5560 | Train Acc: 0.4844 | Val Loss: 5.2186 | Val Acc: 0.2818\n",
            "Epoch 51 | Train Loss: 3.5476 | Train Acc: 0.4856 | Val Loss: 5.2097 | Val Acc: 0.2808\n",
            "Epoch 52 | Train Loss: 3.5576 | Train Acc: 0.4864 | Val Loss: 5.2174 | Val Acc: 0.2823\n",
            "Epoch 53 | Train Loss: 3.5305 | Train Acc: 0.4905 | Val Loss: 5.2209 | Val Acc: 0.2836\n",
            "Epoch 54 | Train Loss: 3.5301 | Train Acc: 0.4904 | Val Loss: 5.2154 | Val Acc: 0.2811\n",
            "Epoch 55 | Train Loss: 3.5190 | Train Acc: 0.4924 | Val Loss: 5.2327 | Val Acc: 0.2822\n",
            "Epoch 56 | Train Loss: 3.4090 | Train Acc: 0.5185 | Val Loss: 5.1945 | Val Acc: 0.2888\n",
            "Epoch 57 | Train Loss: 3.3758 | Train Acc: 0.5257 | Val Loss: 5.2026 | Val Acc: 0.2909\n",
            "Epoch 58 | Train Loss: 3.3634 | Train Acc: 0.5283 | Val Loss: 5.2073 | Val Acc: 0.2900\n",
            "Epoch 59 | Train Loss: 3.3522 | Train Acc: 0.5307 | Val Loss: 5.2078 | Val Acc: 0.2893\n",
            "Epoch 60 | Train Loss: 3.3482 | Train Acc: 0.5310 | Val Loss: 5.2089 | Val Acc: 0.2918\n",
            "Epoch 61 | Train Loss: 3.2805 | Train Acc: 0.5486 | Val Loss: 5.2090 | Val Acc: 0.2930\n",
            "Epoch 62 | Train Loss: 3.2659 | Train Acc: 0.5524 | Val Loss: 5.2078 | Val Acc: 0.2912\n",
            "Epoch 63 | Train Loss: 3.2616 | Train Acc: 0.5528 | Val Loss: 5.2083 | Val Acc: 0.2921\n",
            "Epoch 64 | Train Loss: 3.2472 | Train Acc: 0.5559 | Val Loss: 5.2135 | Val Acc: 0.2905\n",
            "Epoch 65 | Train Loss: 3.2127 | Train Acc: 0.5672 | Val Loss: 5.2155 | Val Acc: 0.2927\n",
            "Epoch 66 | Train Loss: 3.2083 | Train Acc: 0.5653 | Val Loss: 5.2156 | Val Acc: 0.2922\n",
            "Epoch 67 | Train Loss: 3.2059 | Train Acc: 0.5675 | Val Loss: 5.2187 | Val Acc: 0.2928\n",
            "Epoch 68 | Train Loss: 3.2048 | Train Acc: 0.5655 | Val Loss: 5.2200 | Val Acc: 0.2932\n",
            "Epoch 69 | Train Loss: 3.1816 | Train Acc: 0.5729 | Val Loss: 5.2227 | Val Acc: 0.2938\n",
            "Epoch 70 | Train Loss: 3.1754 | Train Acc: 0.5743 | Val Loss: 5.2273 | Val Acc: 0.2920\n",
            "Epoch 71 | Train Loss: 3.1724 | Train Acc: 0.5753 | Val Loss: 5.2288 | Val Acc: 0.2922\n",
            "Epoch 72 | Train Loss: 3.1732 | Train Acc: 0.5738 | Val Loss: 5.2255 | Val Acc: 0.2929\n",
            "Epoch 73 | Train Loss: 3.1651 | Train Acc: 0.5766 | Val Loss: 5.2292 | Val Acc: 0.2920\n",
            "Epoch 74 | Train Loss: 3.1655 | Train Acc: 0.5754 | Val Loss: 5.2287 | Val Acc: 0.2930\n",
            "Epoch 75 | Train Loss: 3.1620 | Train Acc: 0.5777 | Val Loss: 5.2300 | Val Acc: 0.2927\n",
            "Epoch 76 | Train Loss: 3.1614 | Train Acc: 0.5772 | Val Loss: 5.2306 | Val Acc: 0.2916\n",
            "Epoch 77 | Train Loss: 3.1547 | Train Acc: 0.5777 | Val Loss: 5.2325 | Val Acc: 0.2917\n",
            "Epoch 78 | Train Loss: 3.1561 | Train Acc: 0.5786 | Val Loss: 5.2315 | Val Acc: 0.2918\n",
            "Epoch 79 | Train Loss: 3.1558 | Train Acc: 0.5799 | Val Loss: 5.2334 | Val Acc: 0.2912\n",
            "Epoch 80 | Train Loss: 3.1559 | Train Acc: 0.5783 | Val Loss: 5.2330 | Val Acc: 0.2912\n",
            "Epoch 81 | Train Loss: 3.1494 | Train Acc: 0.5804 | Val Loss: 5.2329 | Val Acc: 0.2915\n",
            "Epoch 82 | Train Loss: 3.1504 | Train Acc: 0.5803 | Val Loss: 5.2341 | Val Acc: 0.2908\n",
            "Epoch 83 | Train Loss: 3.1486 | Train Acc: 0.5805 | Val Loss: 5.2330 | Val Acc: 0.2910\n",
            "Epoch 84 | Train Loss: 3.1471 | Train Acc: 0.5800 | Val Loss: 5.2341 | Val Acc: 0.2911\n",
            "Epoch 85 | Train Loss: 3.1457 | Train Acc: 0.5803 | Val Loss: 5.2346 | Val Acc: 0.2912\n",
            "Epoch 86 | Train Loss: 3.1486 | Train Acc: 0.5803 | Val Loss: 5.2347 | Val Acc: 0.2910\n",
            "Epoch 87 | Train Loss: 3.1440 | Train Acc: 0.5827 | Val Loss: 5.2346 | Val Acc: 0.2912\n",
            "Epoch 88 | Train Loss: 3.1476 | Train Acc: 0.5814 | Val Loss: 5.2350 | Val Acc: 0.2910\n",
            "Epoch 89 | Train Loss: 3.1472 | Train Acc: 0.5815 | Val Loss: 5.2348 | Val Acc: 0.2910\n",
            "Epoch 90 | Train Loss: 3.1467 | Train Acc: 0.5804 | Val Loss: 5.2345 | Val Acc: 0.2912\n",
            "Epoch 91 | Train Loss: 3.1462 | Train Acc: 0.5811 | Val Loss: 5.2344 | Val Acc: 0.2912\n",
            "Epoch 92 | Train Loss: 3.1469 | Train Acc: 0.5815 | Val Loss: 5.2347 | Val Acc: 0.2913\n",
            "Epoch 93 | Train Loss: 3.1432 | Train Acc: 0.5803 | Val Loss: 5.2346 | Val Acc: 0.2912\n",
            "Epoch 94 | Train Loss: 3.1469 | Train Acc: 0.5807 | Val Loss: 5.2346 | Val Acc: 0.2912\n",
            "Epoch 95 | Train Loss: 3.1462 | Train Acc: 0.5807 | Val Loss: 5.2345 | Val Acc: 0.2914\n",
            "Epoch 96 | Train Loss: 3.1460 | Train Acc: 0.5824 | Val Loss: 5.2347 | Val Acc: 0.2914\n",
            "Epoch 97 | Train Loss: 3.1462 | Train Acc: 0.5808 | Val Loss: 5.2348 | Val Acc: 0.2914\n",
            "Epoch 98 | Train Loss: 3.1472 | Train Acc: 0.5811 | Val Loss: 5.2346 | Val Acc: 0.2914\n",
            "Epoch 99 | Train Loss: 3.1528 | Train Acc: 0.5798 | Val Loss: 5.2346 | Val Acc: 0.2913\n",
            "Epoch 100 | Train Loss: 3.1482 | Train Acc: 0.5794 | Val Loss: 5.2345 | Val Acc: 0.2914\n",
            "Model saved as 'sherlock_lstm_attention_pytorch_stacked.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"sherlock_lstm_attention_pytorch_stacked.pth\")\n"
      ],
      "metadata": {
        "id": "BgXmxZcqWBbe"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, seed_text, max_length=50, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text from a seed text by iteratively predicting next tokens.\n",
        "    For each predicted token, print the top 5 most likely next tokens with probabilities.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LSTM attention model\n",
        "        seed_text (str): Initial text prompt to start generation\n",
        "        max_length (int): Maximum length of generated tokens (including seed)\n",
        "        temperature (float): Sampling temperature for controlling randomness\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text sequence including the seed_text\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize seed text\n",
        "    tokens = seed_text.lower().split()\n",
        "    sequence = [start_idx]  # Start with <START> token\n",
        "\n",
        "    # Map seed tokens to indices (using <UNK> if not found)\n",
        "    for token in tokens:\n",
        "        sequence.append(word2idx.get(token, word2idx['<UNK>']))\n",
        "\n",
        "    generated_tokens = tokens.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length - len(sequence) + 1):  # Adjust for seed length\n",
        "            # Prepare input: last max_seq_len tokens, padded if needed\n",
        "            if len(sequence) < max_seq_len:\n",
        "                padding = [0] * (max_seq_len - len(sequence))\n",
        "                input_seq = torch.tensor([padding + sequence]).to(device)\n",
        "            else:\n",
        "                input_seq = torch.tensor([sequence[-max_seq_len:]]).to(device)\n",
        "\n",
        "            # Get model output logits\n",
        "            output, _ = model(input_seq)\n",
        "            logits = output[0] / temperature\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "            # Get top 5 predicted tokens and probabilities\n",
        "            top5_probs, top5_idx = torch.topk(probs, 5)\n",
        "            top5_words = [idx2word[idx.item()] for idx in top5_idx]\n",
        "\n",
        "            # Print top 5 predictions with probabilities\n",
        "            print(f\"Top 5 next words: {[(w, float(p)) for w, p in zip(top5_words, top5_probs)]}\")\n",
        "\n",
        "            # Sample next token from probability distribution\n",
        "            next_token_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Stop if EOS token generated\n",
        "            if next_token_idx == eos_idx:\n",
        "                break\n",
        "\n",
        "            # Append predicted token to sequence and generated output tokens\n",
        "            sequence.append(next_token_idx)\n",
        "            next_word = idx2word.get(next_token_idx, '<UNK>')\n",
        "\n",
        "            # Avoid adding special tokens to generated text output\n",
        "            if next_word not in ['<PAD>', '<UNK>', START_TOKEN]:\n",
        "                generated_tokens.append(next_word)\n",
        "\n",
        "    return ' '.join(generated_tokens)\n"
      ],
      "metadata": {
        "id": "rNrhvnyueVOZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For trial\n",
        "def generate_text_beam_search(model, seed_text, max_length=50, beam_width=3):\n",
        "    \"\"\"\n",
        "    Generate text using beam search instead of greedy decoding or sampling.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        seed_text (str): Initial seed prompt\n",
        "        max_length (int): Max number of tokens in the output\n",
        "        beam_width (int): Number of beams to keep at each step\n",
        "\n",
        "    Returns:\n",
        "        str: Best generated sequence\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    tokens = seed_text.lower().split()\n",
        "\n",
        "    # Initial sequence\n",
        "    initial_sequence = [start_idx] + [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
        "    initial_sequence = initial_sequence[-max_seq_len:]\n",
        "\n",
        "    # Pad if needed\n",
        "    if len(initial_sequence) < max_seq_len:\n",
        "        initial_sequence = [0] * (max_seq_len - len(initial_sequence)) + initial_sequence\n",
        "\n",
        "    # Beams are tuples of (sequence, log_prob)\n",
        "    beams = [(initial_sequence, 0)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            all_candidates = []\n",
        "\n",
        "            for seq, score in beams:\n",
        "                input_seq = torch.tensor([seq[-max_seq_len:]]).to(device)\n",
        "                output, _ = model(input_seq)\n",
        "                logits = output[0]\n",
        "                log_probs = torch.log_softmax(logits, dim=0)\n",
        "\n",
        "                # Get top `beam_width` next tokens\n",
        "                top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    next_token = top_indices[i].item()\n",
        "                    next_score = score + top_log_probs[i].item()\n",
        "                    new_seq = seq + [next_token]\n",
        "\n",
        "                    if next_token == eos_idx:\n",
        "                        # Stop early if EOS token predicted\n",
        "                        all_candidates.append((new_seq, next_score))\n",
        "                    else:\n",
        "                        all_candidates.append((new_seq, next_score))\n",
        "\n",
        "            # Keep only top beam_width candidates\n",
        "            beams = sorted(all_candidates, key=lambda tup: tup[1], reverse=True)[:beam_width]\n",
        "\n",
        "            # Early stopping if all beams have ended in EOS\n",
        "            if all(seq[-1] == eos_idx for seq, _ in beams):\n",
        "                break\n",
        "\n",
        "    # Choose the best beam\n",
        "    best_sequence, _ = beams[0]\n",
        "\n",
        "    # Convert indices to tokens, skipping special tokens\n",
        "    generated_tokens = [\n",
        "        idx2word[idx] for idx in best_sequence\n",
        "        if idx not in [word2idx.get('<PAD>', -1), word2idx.get('<UNK>', -1), start_idx, eos_idx]\n",
        "    ]\n",
        "\n",
        "    return ' '.join(generated_tokens)\n"
      ],
      "metadata": {
        "id": "q5AQlbc0prGn"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying with beam search"
      ],
      "metadata": {
        "id": "qjFfacDA12Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"Holmes is \",\n",
        "    \"Watson was\",\n",
        "    \"The detective\",\n",
        "    \"I saw\",\n",
        "    \"It was a dark\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text_beam_search(model, prompt, max_length=20)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: '{generated}'\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srUT74V3psQ9",
        "outputId": "f4a162c4-584d-4cc6-c4bf-04cd336f1c96"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Holmes is '\n",
            "Generated: 'holmes is a very pretty girl and has given him out of the room.'\n",
            "----------------------------------------\n",
            "Prompt: 'Watson was'\n",
            "Generated: 'watson was so then, that i should be able to post me up. too.'\n",
            "----------------------------------------\n",
            "Prompt: 'The detective'\n",
            "Generated: 'the detective which i have already made up my mind that i had returned to civil practice and took a few words'\n",
            "----------------------------------------\n",
            "Prompt: 'I saw'\n",
            "Generated: 'i saw that it was the most preposterous position which had ever seen that she had left the young man had left'\n",
            "----------------------------------------\n",
            "Prompt: 'It was a dark'\n",
            "Generated: 'it was a dark that i had not yet returned.'\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentences do not make much more sense than the ones with greedy.\n"
      ],
      "metadata": {
        "id": "nb8y0QzB17HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we try the greedy decoding whose results I have pasted in the notebook."
      ],
      "metadata": {
        "id": "-k1nkeT32AzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"Holmes is \",\n",
        "    \"Watson was\",\n",
        "    \"The detective\",\n",
        "    \"I saw\",\n",
        "    \"It was a dark\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(model, prompt, max_length=20, temperature=0.8)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: '{generated}'\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW_iLf78erNG",
        "outputId": "46386fbc-c18b-4f4d-bb3e-6442434d2c07"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 next words: [('a', 0.5823346972465515), ('the', 0.066647469997406), ('that', 0.045102473348379135), ('an', 0.03783230856060982), ('very', 0.031026897951960564)]\n",
            "Top 5 next words: [('very', 0.26571258902549744), ('little', 0.19101107120513916), ('small', 0.12731096148490906), ('man', 0.08128730207681656), ('fierce', 0.0749133825302124)]\n",
            "Top 5 next words: [('heavy', 0.22029222548007965), ('pretty', 0.19918301701545715), ('little', 0.19044387340545654), ('large', 0.06172473356127739), ('very', 0.037800583988428116)]\n",
            "Top 5 next words: [('and', 0.8441948294639587), ('between', 0.062399979680776596), ('sleeper,', 0.04337615519762039), ('which', 0.01718759350478649), ('with', 0.007617570459842682)]\n",
            "Top 5 next words: [('darkness', 0.13406139612197876), ('walked', 0.10114217549562454), ('heavy', 0.06598377972841263), ('held', 0.018099963665008545), ('iron', 0.017817718908190727)]\n",
            "Top 5 next words: [('the', 0.4372996985912323), ('a', 0.1787565052509308), ('his', 0.04859798401594162), ('him', 0.04081470146775246), ('it', 0.03885069862008095)]\n",
            "Top 5 next words: [('little', 0.38716986775398254), ('of', 0.12744879722595215), ('small', 0.04793544113636017), ('link', 0.027646662667393684), ('man', 0.02360580489039421)]\n",
            "Top 5 next words: [('as', 0.14678190648555756), ('there', 0.050766926258802414), ('through', 0.045603539794683456), ('a', 0.039651889353990555), ('so', 0.038531530648469925)]\n",
            "Top 5 next words: [('a', 0.4313310384750366), ('one', 0.13898111879825592), ('the', 0.060325492173433304), ('to', 0.05426760017871857), ('his', 0.053641095757484436)]\n",
            "Top 5 next words: [('goose', 0.19888675212860107), ('little', 0.059813980013132095), ('white', 0.057951152324676514), ('link', 0.049473222345113754), ('man', 0.049387238919734955)]\n",
            "Top 5 next words: [('goose', 0.5769937038421631), ('bent', 0.1453474909067154), ('brown', 0.06134522706270218), ('cheeks', 0.01407970953732729), ('link', 0.01229474414139986)]\n",
            "Top 5 next words: [('slung', 0.5461320281028748), ('in', 0.17337967455387115), ('upon', 0.14123116433620453), ('of', 0.05041402950882912), ('to', 0.014478065073490143)]\n",
            "Top 5 next words: [('his', 0.49084237217903137), ('the', 0.3375875651836395), ('slung', 0.021232405677437782), ('all', 0.012349463999271393), ('a', 0.011719029396772385)]\n",
            "Top 5 next words: [('face', 0.16526374220848083), ('cheeks', 0.0711536556482315), ('voice', 0.046916086226701736), ('shoulder.', 0.040754564106464386), ('head', 0.036930229514837265)]\n",
            "Top 5 next words: [('head.', 0.9959756731987), ('arm', 0.0016433369601145387), ('impression', 0.00033926410833373666), ('table,', 0.0002466535079292953), ('and', 0.00019402198086027056)]\n",
            "Top 5 next words: [('<EOS>', 0.9986769556999207), ('of', 7.104963879100978e-05), ('head.', 1.3238919564173557e-05), ('little', 1.3058602235105354e-05), ('chink', 9.392278116138186e-06)]\n",
            "Prompt: 'Holmes is '\n",
            "Generated: 'holmes is a very heavy and opened some close-fitting as a white goose upon his flaming head.'\n",
            "----------------------------------------\n",
            "Top 5 next words: [('so', 0.10546635091304779), ('but', 0.08663862943649292), ('it', 0.08341487497091293), ('in', 0.06048685684800148), ('not', 0.05543135106563568)]\n",
            "Top 5 next words: [('the', 0.4040200710296631), ('that', 0.11268283426761627), ('one', 0.07684382051229477), ('it', 0.039939556270837784), ('he', 0.02685757912695408)]\n",
            "Top 5 next words: [('moment', 0.600499153137207), ('<UNK>', 0.04777751490473747), ('first', 0.03908785060048103), ('last', 0.0295556653290987), ('same', 0.021041089668869972)]\n",
            "Top 5 next words: [('of', 0.463861882686615), ('when', 0.24241556227207184), ('or', 0.09381761401891708), ('then,', 0.04345019534230232), ('and', 0.03519059717655182)]\n",
            "Top 5 next words: [('the', 0.8559945821762085), ('my', 0.03932814300060272), ('mr.', 0.026622071862220764), ('his', 0.018430126830935478), ('your', 0.014034280553460121)]\n",
            "Top 5 next words: [('last', 0.5401196479797363), ('pool', 0.06053681671619415), ('door,', 0.018975136801600456), ('house,', 0.01414518989622593), ('<UNK>', 0.0134846530854702)]\n",
            "Top 5 next words: [('london', 0.22870583832263947), ('few', 0.20850770175457), ('states', 0.09742055833339691), ('single', 0.07828395813703537), ('witness.', 0.03195361793041229)]\n",
            "Top 5 next words: [('but', 0.5391944050788879), ('said', 0.1079741045832634), ('i.', 0.07954771816730499), ('for', 0.02788044884800911), ('as', 0.02153349481523037)]\n",
            "Top 5 next words: [('the', 0.12480776011943817), ('i.', 0.11675642430782318), ('holmes', 0.09283915907144547), ('my', 0.08869172632694244), ('i', 0.05785762891173363)]\n",
            "Top 5 next words: [('<EOS>', 0.9941174983978271), ('the', 0.0030809598974883556), ('my', 0.000953257898800075), ('your', 0.0003142689820379019), ('a', 0.00012030736252199858)]\n",
            "Prompt: 'Watson was'\n",
            "Generated: 'watson was at the moment of the last already,' but i.'\n",
            "----------------------------------------\n",
            "Top 5 next words: [('and', 0.15466643869876862), ('of', 0.1339583843946457), ('which', 0.06865507364273071), ('about', 0.04625558853149414), ('were', 0.03240438923239708)]\n",
            "Top 5 next words: [('during', 0.13745912909507751), ('just', 0.1156328096985817), ('and', 0.1106153205037117), ('but', 0.09543406218290329), ('of', 0.0400993674993515)]\n",
            "Top 5 next words: [('and', 0.4501616656780243), ('as', 0.17216673493385315), ('however,', 0.09286846965551376), ('came', 0.04033590108156204), ('but', 0.038192905485630035)]\n",
            "Top 5 next words: [('i', 0.17947418987751007), ('the', 0.14824120700359344), ('we', 0.1295718252658844), ('my', 0.07865646481513977), ('a', 0.06920323520898819)]\n",
            "Top 5 next words: [('have', 0.35155144333839417), ('may', 0.14480505883693695), ('had', 0.09444618970155716), ('came', 0.08784738928079605), ('could', 0.07874701917171478)]\n",
            "Top 5 next words: [('been', 0.5048691034317017), ('heard', 0.31240150332450867), ('made', 0.031214535236358643), ('heard,', 0.02805810049176216), ('already', 0.012578816153109074)]\n",
            "Top 5 next words: [('that', 0.4356001019477844), ('my', 0.11887266486883163), ('of', 0.09298993647098541), ('some', 0.06741493195295334), ('the', 0.06338461488485336)]\n",
            "Top 5 next words: [('sound', 0.10234363377094269), ('man', 0.08601086586713791), ('little', 0.07794704288244247), ('young', 0.07432398200035095), ('only', 0.06572292745113373)]\n",
            "Top 5 next words: [('of', 0.9993659853935242), ('and', 0.0002653866831678897), ('which', 0.00013683154247701168), ('in', 0.00012962859182152897), ('man', 3.366915916558355e-05)]\n",
            "Top 5 next words: [('man', 0.3339461386203766), ('events,', 0.22399993240833282), ('of', 0.02377653308212757), ('<UNK>', 0.022678643465042114), ('our', 0.021177398040890694)]\n",
            "Top 5 next words: [('which', 0.4415261447429657), ('and', 0.402180552482605), ('the', 0.02942156046628952), ('our', 0.027833785861730576), ('with', 0.01310028973966837)]\n",
            "Top 5 next words: [('are', 0.1959277242422104), ('was', 0.1335587352514267), ('were', 0.12390213459730148), ('is', 0.09728418290615082), ('had', 0.09702048450708389)]\n",
            "Top 5 next words: [('be', 0.4519069194793701), ('have', 0.19355995953083038), ('do', 0.042333681136369705), ('speak', 0.035690031945705414), ('cover', 0.017350593581795692)]\n",
            "Top 5 next words: [('to', 0.5314347147941589), ('in', 0.1042737141251564), ('into', 0.07985417544841766), ('from', 0.07463087141513824), ('upon', 0.05639507994055748)]\n",
            "Top 5 next words: [('the', 0.6482970118522644), ('his', 0.15429265797138214), ('your', 0.07224328070878983), ('you.', 0.051639024168252945), ('my', 0.010872597806155682)]\n",
            "Top 5 next words: [('advertisement', 0.1830718219280243), ('general', 0.09077607840299606), ('matter.', 0.08853551745414734), ('official', 0.06700880825519562), ('<UNK>', 0.03810931369662285)]\n",
            "Top 5 next words: [('and', 0.7410340905189514), ('from', 0.056823957711458206), ('sheet', 0.03276922181248665), ('which', 0.024427657946944237), ('with', 0.021571563556790352)]\n",
            "Top 5 next words: [('of', 0.13930103182792664), ('which', 0.10824467986822128), ('to', 0.08732911944389343), ('in', 0.057747453451156616), ('from', 0.053705330938100815)]\n",
            "Prompt: 'The detective'\n",
            "Generated: 'the detective subduing just as i have heard the love of man which would speak to the advertisement and paper'\n",
            "----------------------------------------\n",
            "Top 5 next words: [('that', 0.5647698044776917), ('my', 0.06084911525249481), ('her', 0.055717166513204575), ('the', 0.05426477640867233), ('it', 0.04079709202051163)]\n",
            "Top 5 next words: [('it', 0.31695473194122314), ('i', 0.25976434350013733), ('he', 0.09226618707180023), ('the', 0.07812374085187912), ('she', 0.06564026325941086)]\n",
            "Top 5 next words: [('was', 0.754633903503418), ('would', 0.14120124280452728), ('had', 0.061569854617118835), ('is', 0.01591992750763893), ('could', 0.010669583454728127)]\n",
            "Top 5 next words: [('not', 0.2814643383026123), ('a', 0.2515329122543335), ('no', 0.09011754393577576), ('nothing', 0.04878195747733116), ('an', 0.0448392853140831)]\n",
            "Top 5 next words: [('one', 0.2333536446094513), ('doubt', 0.12279751896858215), ('great', 0.11470913141965866), ('sign', 0.06362618505954742), ('wonder', 0.04464488849043846)]\n",
            "Top 5 next words: [('of', 0.4146765470504761), ('pass', 0.09104754030704498), ('to', 0.08847516775131226), ('about', 0.08743906021118164), ('which', 0.07698241621255875)]\n",
            "Top 5 next words: [('to', 0.2827402949333191), ('for', 0.2044687122106552), ('is', 0.09733148664236069), ('in', 0.07170431315898895), ('came', 0.0670996680855751)]\n",
            "Top 5 next words: [('get', 0.35676372051239014), ('go', 0.11304490268230438), ('make', 0.1037227138876915), ('tell', 0.08146451413631439), ('find', 0.0733337551355362)]\n",
            "Top 5 next words: [('the', 0.3542419373989105), ('your', 0.3296046257019043), ('it', 0.057715240865945816), ('any', 0.057624220848083496), ('my', 0.04694584012031555)]\n",
            "Top 5 next words: [('<UNK>', 0.17107148468494415), ('matter', 0.13178420066833496), ('man', 0.06602466851472855), ('case', 0.05144929885864258), ('easy', 0.05045687407255173)]\n",
            "Top 5 next words: [('up,', 0.599224328994751), ('of', 0.06435661017894745), ('in', 0.05586996302008629), ('to', 0.04728386923670769), ('<UNK>', 0.046214886009693146)]\n",
            "Top 5 next words: [('said', 0.5078865885734558), ('<UNK>', 0.06149223446846008), ('but', 0.05340610072016716), ('\"of', 0.030518395826220512), ('at', 0.02939550019800663)]\n",
            "Top 5 next words: [('he,', 0.9009114503860474), ('he.', 0.030760303139686584), ('i,', 0.027820590883493423), ('i.', 0.014393763616681099), ('i;', 0.007851573638617992)]\n",
            "Top 5 next words: [(\"'and\", 0.16060025990009308), ('\"of', 0.1309751570224762), ('\"but', 0.11069761961698532), ('smiling,', 0.10203079134225845), ('turning', 0.06453414261341095)]\n",
            "Top 5 next words: [('me,', 0.26354897022247314), ('into', 0.15451517701148987), ('open', 0.09770163148641586), ('to', 0.08828584849834442), ('me', 0.051548171788454056)]\n",
            "Top 5 next words: [('that', 0.14746078848838806), ('mr.', 0.14279575645923615), ('and', 0.1311279535293579), ('until', 0.06747191399335861), ('dark', 0.0477406270802021)]\n",
            "Top 5 next words: [('and', 0.30031687021255493), ('to', 0.2414587140083313), ('in', 0.22068041563034058), ('as', 0.06032709777355194), ('or', 0.03511640802025795)]\n",
            "Top 5 next words: [('eyes,', 0.11159378290176392), ('so', 0.09509867429733276), ('happened', 0.09263879805803299), ('servant', 0.07431317120790482), ('dropped', 0.03377898782491684)]\n",
            "Prompt: 'I saw'\n",
            "Generated: 'i saw that it is no one else to make the matter up,\" said he, turning me, dark and eyes,'\n",
            "----------------------------------------\n",
            "Top 5 next words: [('that', 0.4273039400577545), ('glance', 0.08492828905582428), ('more', 0.08106835186481476), ('faced', 0.043214112520217896), ('room,', 0.03990040719509125)]\n",
            "Top 5 next words: [('that', 0.7264592051506042), ('i', 0.15431906282901764), ('room,', 0.017399631440639496), ('little', 0.010366713628172874), ('vague', 0.006679899524897337)]\n",
            "Top 5 next words: [('i', 0.2595432996749878), ('he', 0.12264027446508408), ('she', 0.08638273924589157), ('my', 0.04873884841799736), ('thought', 0.04189915210008621)]\n",
            "Top 5 next words: [('when', 0.12350807338953018), ('was', 0.11508574336767197), ('in', 0.041366394609212875), ('left', 0.037642695009708405), ('had', 0.03551981598138809)]\n",
            "Top 5 next words: [('however,', 0.3512680232524872), ('but', 0.19248853623867035), ('<EOS>', 0.1711045801639557), ('at', 0.07272770255804062), ('i', 0.037161972373723984)]\n",
            "Top 5 next words: [('the', 0.5099672079086304), ('last,', 0.06991942971944809), ('a', 0.0454423725605011), ('see', 0.04003835842013359), ('only', 0.03811102360486984)]\n",
            "Top 5 next words: [(\"o'clock\", 0.3879232108592987), ('minutes', 0.2242884486913681), ('same', 0.059238579124212265), ('time', 0.035461533814668655), ('i', 0.018669921904802322)]\n",
            "Top 5 next words: [('i', 0.3528241813182831), ('she', 0.19774888455867767), ('he', 0.18308746814727783), ('we', 0.05490503087639809), ('but', 0.051822956651449203)]\n",
            "Top 5 next words: [('man', 0.34460315108299255), ('she', 0.07769852131605148), ('i', 0.04614279791712761), ('cheetah', 0.03304464370012283), ('door', 0.032779548317193985)]\n",
            "Top 5 next words: [('had', 0.5627781748771667), ('has', 0.09319290518760681), ('is', 0.07241788506507874), ('was', 0.06189895421266556), ('endeavoured', 0.04779623821377754)]\n",
            "Top 5 next words: [('been', 0.2668614685535431), ('now', 0.12724153697490692), ('just', 0.12455647438764572), ('had', 0.06490432471036911), ('made', 0.04399166256189346)]\n",
            "Top 5 next words: [('as', 0.2085222452878952), ('been', 0.17991039156913757), ('that', 0.12364010512828827), ('now', 0.06260184943675995), ('one', 0.05030852556228638)]\n",
            "Top 5 next words: [('as', 0.2791258692741394), ('one.', 0.09961295127868652), ('in', 0.06425266712903976), ('<UNK>', 0.044653620570898056), ('to', 0.04148653894662857)]\n",
            "Top 5 next words: [('<EOS>', 0.9999008178710938), ('i', 2.393215982010588e-05), ('it', 1.2523414625320584e-05), ('a', 7.579886187158991e-06), ('there', 2.506245664335438e-06)]\n",
            "Prompt: 'It was a dark'\n",
            "Generated: 'it was a dark so that railway marriage at ten o'clock a noise had just been one.'\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate perplexity for all datasets\n",
        "train_perplexity = calculate_perplexity(model, train_loader, device)\n",
        "val_perplexity = calculate_perplexity(model, val_loader, device)\n",
        "test_perplexity = calculate_perplexity(model, test_loader, device)\n",
        "\n",
        "print(f\"PERPLEXITY SCORES:\")\n",
        "print(f\"Training Perplexity: {train_perplexity:.2f}\")\n",
        "print(f\"Validation Perplexity: {val_perplexity:.2f}\")\n",
        "print(f\"Test Perplexity: {test_perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhbpQQOZlUHo",
        "outputId": "f092d735-11d8-4097-e8e2-8e634c947496"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PERPLEXITY SCORES:\n",
            "Training Perplexity: 4.07\n",
            "Validation Perplexity: 80.79\n",
            "Test Perplexity: 82.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"\\nTEXT GENERATION EXAMPLES:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Test text generation\n",
        "test_prompts = [\n",
        "    \"Holmes said\",\n",
        "    \"Watson was\",\n",
        "    \"The detective\",\n",
        "    \"I saw\",\n",
        "    \"It was a dark\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generated = generate_text(model, prompt, max_length=20, temperature=0.8)\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "    print(f\"Generated: '{generated}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DthZDHJ3V5v-",
        "outputId": "e38e38ba-4148-43a0-cce2-929d5371cd51"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TEXT GENERATION EXAMPLES:\n",
            "----------------------------------------\n",
            "Prompt: 'Holmes said'\n",
            "Generated: 'holmes said that we were engaged in the cellar, and so he stood with a one of a small one, and was'\n",
            "----------------------------------------\n",
            "Prompt: 'Watson was'\n",
            "Generated: 'watson was not'\n",
            "----------------------------------------\n",
            "Prompt: 'The detective'\n",
            "Generated: 'the detective bag with a long made a man in a low voice, it was a something which that was a week'\n",
            "----------------------------------------\n",
            "Prompt: 'I saw'\n",
            "Generated: 'i saw that it would swim sink. i rate at my'\n",
            "----------------------------------------\n",
            "Prompt: 'It was a dark'\n",
            "Generated: 'it was a dark little by that that she had left me, but there i was a marriage to have stiff.\" and the more'\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, data_loader, device):\n",
        "    \"\"\"Calculate top-1 accuracy on a dataset\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)  # [batch_size, vocab_size]\n",
        "            predictions = torch.argmax(outputs, dim=1)  # [batch_size]\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "4o-slPBCooB7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy on test set\n",
        "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
        "\n",
        "print(f\"TEST ACCURACY:\")\n",
        "print(f\"Top-1 Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWqPqxMgpQt1",
        "outputId": "4b2d0419-27b3-4eaa-cd28-cd4ee0e7469f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST ACCURACY:\n",
            "Top-1 Accuracy: 29.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating later, from a loaded model"
      ],
      "metadata": {
        "id": "m6yxRUyc2yA5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4a5299f",
        "outputId": "8b9405e5-f4d1-48a6-8988-ca9d89b90e31"
      },
      "source": [
        "# Load the model state dictionary\n",
        "model = LSTMAttentionModel(len(vocab)).to(device) # Re-instantiate the model\n",
        "model.load_state_dict(torch.load(\"sherlock_lstm_attention_pytorch_stacked.pth\"))\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"✅ Model loaded successfully from 'sherlock_lstm_attention_pytorch_stacked.pth'\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model loaded successfully from 'sherlock_lstm_attention_pytorch_stacked.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb5ff2c",
        "outputId": "fcd7aca6-2107-47e6-a7e8-49b340458007"
      },
      "source": [
        "seed_text = \"It was dark\"\n",
        "generated_text = generate_text(model, seed_text, max_length=20, temperature=0.7)\n",
        "print(f\"\\nGenerated text: {generated_text}\")\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text: it was dark when he went up to the other us in the name of the flight, but at the all he was\n"
          ]
        }
      ]
    }
  ]
}